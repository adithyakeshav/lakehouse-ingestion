# CDC Ingestion: PostgreSQL â†’ Delta Lake (Bronze Layer)
#
# This pipeline ingests CDC events from Kafka (pushed by Kafka Connect/Debezium)
# and writes them to Delta Lake format on MinIO S3.
#
# Infrastructure:
# - Source: Kafka topic postgres_cdc_music_school_db
# - Target: Delta Lake on MinIO S3 (s3a://lakehouse/bronze/cdc/)
# - Catalog: Hive Metastore
# - Format: Delta Lake (for ACID and time travel)

env = "dev"

jobs = [
  {
    domain = "music_school"
    dataset = "cdc"

    source = {
      type = "kafka"
      options = {
        # Kafka broker (internal Kubernetes service)
        bootstrap.servers = "datalake-kafka-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092"

        # CDC topic from Kafka Connect
        subscribe = "postgres_cdc_music_school_db"

        # Streaming mode
        streaming = "true"
        startingOffsets = "earliest"  # For dev/testing, use "latest" in prod

        # Consumer group
        group.id = "lakehouse-ingestion-cdc-music-school"

        # Kafka settings
        kafka.security.protocol = "PLAINTEXT"
        failOnDataLoss = "false"  # Don't fail if topic is recreated

        # Performance tuning
        maxOffsetsPerTrigger = "10000"  # Limit records per micro-batch
        minPartitions = "2"  # Parallelism
      }
    }

    target = {
      # Delta Lake path on MinIO S3
      # Format: s3a://bucket/path
      table = "s3a://lakehouse/bronze/music_school/cdc/"

      lakehouse_format = "delta"
      catalog = "hive"
      layer = "bronze"

      # Partition by date for efficient querying
      # Extract date from ts_ms field in transformation (future)
      partitions = []  # No partitioning for now, add later if needed
    }

    schema = {
      registry_domain = "music_school"
      registry_dataset = "cdc"
      version = "v1"
    }

    data_quality = {
      # For CDC bronze layer, just log issues - don't fail
      # CDC data should be preserved as-is
      on_fail = "LOG_ONLY"
    }
  }
]
