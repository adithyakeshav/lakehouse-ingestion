# Example: Multi-job pipeline configuration
#
# This example demonstrates:
# - Multiple ingestion jobs in single config
# - Different source types
# - Different lakehouse formats
# - Different DQ policies per job

env = "production"

jobs = [
  # Job 1: Payments transactions (critical data - FAIL_FAST)
  {
    domain = "payments"
    dataset = "transactions"

    source = {
      type = "kafka"
      options = {
        bootstrap.servers = "kafka.prod:9092"
        subscribe = "payments.transactions"
        startingOffsets = "latest"
        streaming = "true"
        group.id = "lakehouse-ingestion-payments"
      }
    }

    target = {
      table = "payments.transactions_bronze"
      lakehouse_format = "iceberg"
      catalog = "hive"
      layer = "bronze"
      partitions = ["transaction_date"]
    }

    schema = {
      registry_domain = "payments"
      registry_dataset = "transactions"
      # version omitted - will use latest
    }

    data_quality = {
      on_fail = "FAIL_FAST"  # Critical data - fail if DQ issues
    }
  },

  # Job 2: User events (non-critical - LOG_ONLY)
  {
    domain = "user_events"
    dataset = "clicks"

    source = {
      type = "kafka"
      options = {
        bootstrap.servers = "kafka.prod:9092"
        subscribe = "user_events.clicks"
        startingOffsets = "latest"
        streaming = "true"
        group.id = "lakehouse-ingestion-user-events"
      }
    }

    target = {
      table = "user_events.clicks_bronze"
      lakehouse_format = "parquet"
      catalog = "hive"
      layer = "bronze"
      partitions = ["event_date"]
    }

    schema = {
      registry_domain = "user_events"
      registry_dataset = "clicks"
      version = "v1"
    }

    data_quality = {
      on_fail = "LOG_ONLY"  # Non-critical - log but continue
    }
  }
]
