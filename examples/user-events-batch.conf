# Example: User click events batch ingestion to Parquet
#
# This example demonstrates:
# - Batch processing (non-streaming)
# - S3/file-based source
# - Parquet output format
# - Data quality checks with QUARANTINE policy
# - Bronze layer ingestion

env = "staging"

jobs = [
  {
    domain = "user_events"
    dataset = "clicks"

    source = {
      type = "kafka"  # Using Kafka in batch mode
      options = {
        bootstrap.servers = "kafka:9092"
        subscribe = "user_events.clicks"
        startingOffsets = "earliest"
        endingOffsets = "latest"
        streaming = "false"  # Batch mode
      }
    }

    target = {
      table = "user_events.clicks_bronze"
      lakehouse_format = "parquet"
      catalog = "hive"
      layer = "bronze"
      partitions = ["event_date"]
    }

    schema = {
      registry_domain = "user_events"
      registry_dataset = "clicks"
      version = "v1"
    }

    data_quality = {
      on_fail = "QUARANTINE"  # Bad rows written to quarantine location
    }
  }
]
